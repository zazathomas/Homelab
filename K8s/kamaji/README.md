  
---

# **Hybrid Kubernetes Homelab**  

This guide details how to build a resilient hybrid Kubernetes cluster by deploying the control plane in a managed OKE cluster using Kamaji and running worker nodes on an on-premises Proxmox machine. By combining these environments, you benefit from the inherent fault tolerance of distributing workloads across cloud and local infrastructures, streamlined disaster recovery through rapid re-provisioning of on-prem nodes, and enhanced security by leveraging managed cloud services for the control plane while maintaining stricter access controls on sensitive data locally.

---

## **1. Deploying Kamaji**  

### **Prerequisites**  
Before proceeding, ensure the following:  
- You have a functional Kubernetes cluster with a Storage class.
- Helm is installed on your system.  
- kubeadm is installed on your system.
- kubectl is installed on your system.

### **Step 1: Install Cert-Manager** (Required Dependency)  

Cert-Manager is a prerequisite for Kamaji. To install it, add the Jetstack Helm repository and deploy Cert-Manager with the required CRDs using the following commands:

```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true
```

### **Step 2: Install Kamaji**  

#### **Add the Clastix Helm Repository**  

```bash
helm repo add clastix https://clastix.github.io/charts
```

#### **Deploy Kamaji**  

```bash
git clone https://github.com/clastix/kamaji
cd kamaji
helm dependency build charts/kamaji
helm install kamaji charts/kamaji -n kamaji-system --create-namespace \
    --set image.tag=latest
```

#### **Verify Installation**  

```bash
helm status kamaji -n kamaji-system
```

---
### **Step 3: Install Kamaji Console(Optional)**  
The Kamaji console provides a visual interface for managing kamaji hosted control planes. Kamaji Console needs a Secret in the Kamaji Management Cluster that holds the configuration and credentials for browser access. This Secret can either be auto-generated by the Helm Chart or created manually (with its name provided during installation). Before installing the console, update the placeholders with your actual values and run the provided commands from your workstation.
```
# console-secret.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: kamaji-console
  namespace: kamaji-system
data:
  # Credentials to login into console
  ADMIN_EMAIL: <email>
  ADMIN_PASSWORD: <password>
  # Secret used to sign the browser session
  JWT_SECRET: <jwtSecret>
  # URL where the console is accessible: https://<hostname>/ui
  NEXTAUTH_URL: <nextAuthUrl>
```

#### **Deploy Console**  

```bash
kubectl apply -f console-secret.yaml
helm -n kamaji-system install console clastix/kamaji-console -f console-values.yaml
```

#### **Verify Installation**  

```bash
helm status console -n kamaji-system
```

Once all resources are available, you may expose the service using port-forwarding, ingress, or a HTTP route(my preferred method) to access the console. Then, log in using the credentials stored in the secret.

---

## **4. Deploy a Sample Workload Control Plane**  

Apply the control plane template:  

```bash
kubectl apply -f tcp-template.yaml
```

### **Retrieve the Kubeconfig for the Workload Cluster**  

To obtain the kubeconfig file for your cluster, you can either run the following command or download it directly from the Kamaji Console:
```bash
export TENANT_NAMESPACE=<specify namespace control plane is deployed>
export TENANT_NAME=<specify control plane name>
kubectl get secrets -n ${TENANT_NAMESPACE} ${TENANT_NAME}-admin-kubeconfig -o json \
  | jq -r '.data["admin.conf"]' \
  | base64 --decode \
  > ${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig
```

---

## **5. Setting Up Worker Nodes on Proxmox**  
 
To bootstrap the worker nodes, I have refined the [Yaki](https://github.com/clastix/yaki/tree/master) script. Please ensure you are using Ubuntu 22.04 or 24.04 before executing the following steps.

```bash
sudo apt install conntrack socat -y
curl -sfL https://raw.githubusercontent.com/zazathomas/Homelab/refs/heads/main/K8s/kamaji/yaki.sh > yaki.sh && chmod +x yaki.sh
sudo KUBERNETES_VERSION=v1.31.4 ./yaki.sh bootstrap  # Ensure the specified kubernetes version matches the control plane version.
rm yaki.sh
```

After the machine is configured, convert it into a template and deploy three worker VMs based on that template. 

---

## **6. Joining Worker Nodes to the Cluster**  

### **Retrieve the Join Command**  

```bash
JOIN_CMD=$(echo "sudo ")$(kubeadm --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig token create --print-join-command)
```

### **Add Worker Nodes to the Cluster**  

```bash
WORKER0=<worker-node-1-ip>
WORKER1=<worker-node-2-ip>
WORKER2=<worker-node-3-ip>

HOSTS=(${WORKER0} ${WORKER1} ${WORKER2})
for i in "${!HOSTS[@]}"; do
  HOST=${HOSTS[$i]}
  ssh ${USER}@${HOST} -t ${JOIN_CMD}
done
```

---

## **7. Installing Core Components on the Workload Cluster**  

Once worker nodes are joined, install essential Kubernetes components:  

- **Container Network Interface (CNI)**
- **Storage Configuration**
- **Metrics Server**

These components ensure proper networking, persistent storage, and resource monitoring in the cluster.  

---

### **Conclusion**  
By following this guide, you have successfully deployed Kamaji, set up worker nodes on Proxmox, and configured a functional hybrid kubernetes cluster. Ensure that all components are running correctly before deploying workloads.  
